{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Wrange and Analyse WeRateDogs Twitter data (part 1/2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Introduction](#intro)\n",
    "2. [Wrangling](#wrangling)     \n",
    "3. [Clean tasks done](#cleaning)\n",
    "4. [Outputs  ](#outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This project focused on wrangling data from the WeRateDogs Twitter account using Python, documented in a Jupyter Notebook (wrangle_act.ipynb). This Twitter account rates dogs with humorous commentary. The rating denominator is usually 10, however, the numerators are usually greater than 10. Theyâ€™re Good Dogs Brent wrangles WeRateDogs Twitter data to create interesting and trustworthy analyses and visualizations. WeRateDogs has over 4 million followers and has received international media coverage.\n",
    "\n",
    "WeRateDogs downloaded their Twitter archive and sent it to Udacity via email exclusively for us to use in this project. This archive contains basic tweet data (tweet ID, timestamp, text, etc.) for all 5000+ of their tweets as they stood on August 1, 2017.\n",
    "\n",
    "The goal of this project is to wrangle the WeRateDogs Twitter data to create interesting and trustworthy analyses and visualizations. The challenge lies in the fact that the Twitter archive is great, but it only contains very basic tweet information that comes in JSON format. I needed to gather, assess and clean the Twitter data for a worthy analysis and visualization.\n",
    "\n",
    "This report focuses on providing an overview of the wrangling and cleaning of the data which is gathered from 3 data sources (see par 1.1.).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Origin of Data sources\n",
    "\n",
    "We gather 3 pieces of data:\n",
    "\n",
    "### a. Enhanced Twitter Archive\n",
    "\n",
    "The WeRateDogs Twitter archive contains basic tweet data for all 5000+ of their tweets, but not everything. One column the archive does contain though: each tweet's text, which I used to extract rating, dog name, and dog \"stage\" (i.e. doggo, floofer, pupper, and puppo) to make this Twitter archive \"enhanced.\". We manually downloaded this twitter_archive_enhanced.csv file by clicking the following [link](https://d17h27t6h515a5.cloudfront.net/topher/2017/August/59a4e958_twitter-archive-enhanced/twitter-archive-enhanced.csv). \n",
    "\n",
    "### b. Image Predictions File\n",
    "\n",
    "The tweet image predictions, i.e., what breed of dog (or other object, animal, etc.) is present in each tweet according to a neural network. This file (image_predictions.tsv) hosted on Udacity's servers and we downloaded it programmatically using python Requests library AND the following URL of the file: https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv)\n",
    "\n",
    "### c. Twitter API\n",
    "\n",
    "Back to the basic-ness of Twitter archives: retweet count and favorite count are two of the notable column omissions. Fortunately, this additional data can be gathered by anyone from Twitter's API. Well, \"anyone\" who has access to data for the 3000 most recent tweets, at least. But we, because we have the WeRateDogs Twitter archive and specifically the tweet IDs within it, can gather this data for all 5000+.\n",
    "In this project, I'll be using Tweepy to query Twitter's API for data included in the WeRateDogs Twitter archive. This data will include retweet count and favorite count.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## 1.2. Key Points for Data Wrangling\n",
    "\n",
    "Before we start, here are a few points to keep in mind when data wrangling for this project:\n",
    "\n",
    "    1) We only want original ratings (no retweets) that have images. Though there are 5000+ tweets in the dataset, not all are dog ratings and some are retweets.\n",
    "\n",
    "    2) Fully assessing and cleaning the entire dataset requires exceptional effort so only a subset of its issues (eight (8) quality issues and two (2) tidiness issues at minimum) need to be assessed and cleaned.\n",
    "\n",
    "    3) Cleaning includes merging individual pieces of data according to the rules of tidy data.\n",
    "\n",
    "    4) The fact that the rating numerators are greater than the denominators does not need to be cleaned. This unique rating system is a big part of the popularity of WeRateDogs.\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.  Relation to other input and output files of the project\n",
    "\n",
    "* inputfiles: see url's in paragraph 1.1.\n",
    "  - `twitter_archive_enhanced.csv` file with basic data of the WeRateDogs tweeets\n",
    "  - `image_predictons.tsv` file with dog breed predictions of neural network\n",
    "* `wrangle_act v1.0_DIRK.ipynb` : this is the extensive notebook with all details about the data gathering, wrangling and cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* output file with the wrangled data of this data wrangling/cleaning exercise of the former bullit:\n",
    "  - twitter_overal_table pandas dataframe saved in the `twitter_archive_master.csv` file\n",
    "  - twitter_overal_table pandas dataframe is also saved in the `bestofrt.db database` file\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the `wrangle_report_DIRK_KADIJK.ipynb` file of this report which describes the wrangling/cleaning efforts and outputs  (thus *THIS REPORT*)\n",
    "* the `act_report_DIRK_KADIJK.ipynb` file which communicates the insight and visualizations produced from the wrangled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<a id='wrangling'></a>\n",
    "# 2. Wrangling \n",
    "\n",
    "Wrangling resulted in the following identified Quality and Tidiness issues:\n",
    "\n",
    "### 2.1. Quality issues\n",
    "Identied Quality issues while wrangling the data from the 3 data sources mentioned in par 1.1.:\n",
    "\n",
    "##### `twitter_archive table`\n",
    "\n",
    "- tweet_ID, in_reply_to_status_id, in_reply_to_user_id, retweeted_status_id, retweeted_status_user_id should be of object (string) datatype, in stead of numeric type (int64 or float64) since numerical operations are not supposed to be applicable for them. [solved]\n",
    "- timestamp and retweeted_status_timestamp should be of 'date' datatype (in stead of 'object') [solved]\n",
    "- huge number of missing values in reply_to_status_id, in_reply_to_user_id, retweeted_status_id, retweeted_status_user_id [solved]\n",
    "\n",
    "- source column contains html tags at beginning ('<a  ') and end ('/a> ') [solved]\n",
    "\n",
    "- issues expanded_url column: \n",
    "  - expanded_urls contains same url multiple times (2, 3, )\n",
    "  - column 'expanded_urls contains url's in non twitter.com domein (e.g.: https://gofundme.com/ydvmve-surgery-for-jax)  [solved]\n",
    "  - 59 NaN values (null items) in expanded_url column \n",
    "  \n",
    "- table contains rows with reply message in the text column -> in case of value in in_reply_to_status_id column, then there are no values in the columns 'url', 'dogname', and 'dogtype'.  [solved]\n",
    "- table contains rows with retweets which implies duplicated data -> in case of value in column 'retweeted_status_id', then there are no values in the columns 'url', 'dogname', and 'dogtype' ... and this while retweets are not required to capture scores and create risk of duplicating scores  (note: no duplicates of tweet_id identified in twitter_archive table. [solved]\n",
    "\n",
    "- rating_numerator and rating_denominator:\n",
    "  - A substantial number of a too high (>50) rating_numerator value is caused by wrong extraction:\n",
    "    - 5 cases are caused because the text is a reply text (to be identified via a non-NaN value in 2nd column). [solved]\n",
    "    - also cases where a value (e.g. 9.75) is wrongly captured from the decimal part (=number after the dot). See for instance the indexes 343, 695, 763, 1712. [solved]\n",
    "    - cases with several dogs on the photo, to be identified with a rating_denominator value which is explainable by the multiple of the # of dogs times the default 10 dominator value. See also the next cel with the value count of the denominator  [solved]\n",
    "    - some cases are caused by extracting a number from a non-score part of the tweet text (e.g. index 516)\n",
    "    - Note: be aware that also 2 cases are caused by a subjective huge rating by the rater\n",
    "  - But also some of the low (<10) values of rating_numerator is caused by wrong extraction:\n",
    "    - cases where a float value is wrongly extracted (the decimal value after the dot is captered); e.g. Bella with index 45 got the score 5 based on the cut off of decimal part of the 13.5 score.  [solved]\n",
    "    - also some are caused because the text is a reply text (in case of value in 2nd column). [solved]\n",
    "    - and some captured a number from a non-score part of the tweet text (index 387).\n",
    "    - some are caused because animal is not a dog (bug a goat: see 229 and 765). No dog stage is then assigned. [solved]\n",
    "    - Note: be aware that also quite some low rating_numerator values are caused by a less positive explicit opinion of the rater\n",
    "  - 23 cases with a rating_denominator values not equal to 10 which are not accurate since value must be 10:\n",
    "     - 2 cases with a denominator value of resp. 2 and 7 (so less than 10) are wrongly extracted. [solved]\n",
    "     - 1 case because of reply message [solved]\n",
    "     - rest of cases due to several dogs on the photo, which explains a rating_denominator value equal to the multiple of the # of dogs times 10 [solved]\n",
    "\n",
    "- dog names have values which are not dognames as: 'None', or 'a', or 'an.' or 'O', 'my' or 'by'  ; note: these names are wrongly extracted from 'text' field\n",
    "  - also one dog name with only capital letters ('JD')  [solved]\n",
    "  - some dognames start with a lowercase letter ==> str.title  [solved]\n",
    "  - 745 'None' values in column 'name'  [solved]\n",
    "- dog stage classification:   \n",
    "  - a lot of 'None' values in the 4 dog stage class columns doggo, pupper, puppo and floofer.  ==> should be 'Null'  [solved]\n",
    "  - there are even 1976 rows have no dog stage classification score whatsoever (84% of all rows)  [partly solved]\n",
    "  - multiple dog stages occurs such as 'doggo puppo', 'doggo pupper', 'doggo floofer' ; e.g. 12 rows with both 'doggo' and 'floofer' filled in as dog stage [solved] <br><br> \n",
    "  \n",
    "\n",
    "##### `image predictions table` (image_df dataframe)\n",
    "- column names not self explanatory (p1, p1_conf, p1_dog); e.g. p2 predicts the dog class   [solved]\n",
    "- values of p1, p2 and p3 (dog breed classification) have same words with uppercase or lowercase, while they are of same breed category ==> should be lowercase (str.lower) [solved]\n",
    "- dog breeds are not consistently named in p1,p2,p3 columns  \n",
    "- tweet_id is of str datatype [solved] <br><br>\n",
    "\n",
    "##### `retweet and favorite count table` based query on API (df_tweet_json dataframe) \n",
    "- tweet_id is an int64 (should be of string object type).  [solved]\n",
    "- retweet_count and favorite_count are of string datatype, and should be integer   [solved]<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Tidiness issues\n",
    "##### `twitter_archive` table\n",
    "- The variable for the dog's stage (dogoo, floofer, pupper, puppo) is spread in different columns ==> can be combined ('melted') in a single 'dog_stage' column  [solved]\n",
    "\n",
    "##### `image predictions` table (image_df dataframe)\n",
    "- This data set is part of the same observational unit as the data in the archive_table df   [solved]\n",
    " \n",
    "\n",
    "##### `retweet and favorite count` table based query on API (df_tweet_json dataframe) \n",
    "- This data set is also part of the same observational unit as the data in the archive_table _df    [solved]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cleaning'></a>\n",
    "# 3. Cleaning tasks done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cleaning of the identified Quality and Tidiness issues is done:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Convert tweet_ID, in_reply_to_status_id, in_reply_to_user_id, retweeted_status_id, retweeted_status_user_id to the object (string) datatype: via the .astype(str) method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convert tweet_ID to the object (string) datatype: via the .astype(str) method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convert 'timestamp' and 'retweet_status_timestamp' to the object (string) datatype: via the .to_datetime method\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* convert tweet_count and favorite_count to datatype integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* replace 'None' values in dog stage columns to an empty '' value via .replace method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create one column 'stage' for the various dog types: doggo, floofer, pupper, puppo, 'doggo, puppo', 'doggo, pupper', 'doggo, floofer' ascolumn name ' type ' with the categorical dtype\n",
    "* Then drop the obsolete dog stage columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Drop rows with a non-null value in the column 'reply_to_status_id\n",
    "* Drop the columns in_reply_to_status_id and in_reply_to_user_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* drop rows with a non-null value in the column 'retweet_status_id\n",
    "* drop the 3 retweet columns retweet_status_user_id and retweeted_status_timestamp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Merge the table twitter_archive_clean with tweet_json_clean table, based on left inner join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* first change datatype of tweet_id in image_df_clean dataframe to str (object)\n",
    "* merge the twitter_json_interim table with image_df_clean table, based on left inner join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* replace columnname (p1, p1_conf, p1_dog, p2 etc) with self explanatory text values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Replace predicted names to same format via method .str.capitalize()\n",
    "* Also convert the column name to same format via method .str.capitalize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Replace the 'names' with silly values as 'a', 'an', 'the', 'None' and other lower case words with NaN in name column by converting to lowercase + .replace method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Extract source url in 'source' column via .split method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Extract rating_numerator as .float value from text field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* filter out the rows with rating_demoninator not equal to 10\n",
    "* filter out the rows with rating_numerator > 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='outputs'></a>\n",
    "# 4. Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of wrangling/cleaning task is the `twitter_overal_table` pandas dataframe and is saved in the following 2 files:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the twitter_overall_table is saved to the `twitter_archive_master.csv file`\n",
    "* the `bestofrt.db` database file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "See the `act_report_DIRK_KADIJK_v1.1.ipynb` file if you are interested in the insights and visualizations produced from these wrangled data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
